{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlkiAglEq5lQNnWV3NUmNT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asmi1711/Bot-Detection-on-X/blob/main/Bot_Detection_on_X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "jK-4QWOCo2xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Analysis Target Variable: account_type (human → 0, bot → 1)\n",
        "\n",
        "Numerical Features: favourites_count, followers_count, friends_count, statuses_count, average_tweets_per_day, account_age_days\n",
        "\n",
        "Boolean Features (Converted to Numeric 0/1): default_profile, default_profile_image, geo_enabled, verified\n",
        "\n",
        "Text Features (Require Vectorization): description, screen_name, location\n",
        "\n",
        "Columns to Drop: Unnamed: 0, created_at, id, profile_image_url, profile_background_image_url, lang (Not relevant for prediction)"
      ],
      "metadata": {
        "id": "xLcSC1QWo6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0bt2uNromU7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/twitter_human_bots_dataset (2).csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = [\n",
        "    \"Unnamed: 0\", \"created_at\", \"id\", \"screen_name\", \"profile_image_url\",\n",
        "    \"profile_background_image_url\", \"location\", \"description\", \"lang\"\n",
        "]\n",
        "df_cleaned = df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Encode categorical target variable\n",
        "df_cleaned[\"account_type\"] = df_cleaned[\"account_type\"].map({\"human\": 0, \"bot\": 1})\n",
        "\n",
        "# Separate features and target\n",
        "X = df_cleaned.drop(columns=[\"account_type\"])\n",
        "y = df_cleaned[\"account_type\"]\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply SMOTE to balance the classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "y_resampled_df = pd.DataFrame(y_resampled, columns=[\"account_type\"])\n",
        "\n",
        "# Concatenate balanced dataset\n",
        "balanced_df = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
        "\n",
        "# Save the balanced dataset\n",
        "balanced_df.to_csv(\"balanced_twitter_dataset.csv\", index=False)\n",
        "\n",
        "print(\"SMOTE applied successfully. Balanced dataset saved as 'balanced_twitter_dataset.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost"
      ],
      "metadata": {
        "id": "EmD5xC_Movgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/balanced_twitter_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"account_type\"])\n",
        "y = df[\"account_type\"]\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.33, random_state=42, stratify=y_train_temp\n",
        ")\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_val = xgb_model.predict(X_val)\n",
        "y_pred_test = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "train_acc = xgb_model.score(X_train, y_train)\n",
        "val_acc = xgb_model.score(X_val, y_val)\n",
        "test_acc = xgb_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "print(\"\\nConfusion Matrix (Validation):\")\n",
        "print(confusion_matrix(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(\"\\nConfusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Feature Importance Visualization\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Plotting the bar graph\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.barh(X.columns, feature_importances, color='black')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Feature Importance from XGBoost Model')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P-eiykcQoxsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "2xv-KRCXosSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/balanced_twitter_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"account_type\"])\n",
        "y = df[\"account_type\"]\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Split the temporary set into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_val = rf_model.predict(X_val)     # Validation predictions\n",
        "y_pred_test = rf_model.predict(X_test)   # Test predictions\n",
        "\n",
        "# Evaluate the model\n",
        "train_acc = rf_model.score(X_train, y_train)\n",
        "val_acc = rf_model.score(X_val, y_val)   # Validation accuracy\n",
        "test_acc = rf_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Classification reports\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Confusion matrices\n",
        "print(\"\\nConfusion Matrix (Validation):\")\n",
        "print(confusion_matrix(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Feature Importance\n",
        "feature_importances = rf_model.feature_importances_\n",
        "print(\"\\nFeature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"{X.columns[i]}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "yvGl9MXmouVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KNN"
      ],
      "metadata": {
        "id": "nqwc8deoo-FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/balanced_twitter_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"account_type\"])\n",
        "y = df[\"account_type\"]\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")  # Initial split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.33, random_state=42, stratify=y_train_temp\n",
        ")  # Split train_temp into train and validation\n",
        "\n",
        "# Initialize the KNN model\n",
        "# n_neighbors = 5 is the default value (you can tune this)\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the model (on the training set)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions (on the validation set)\n",
        "y_pred_val = knn_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model (on the validation set)\n",
        "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "train_acc = knn_model.score(X_train, y_train)  # Training accuracy\n",
        "val_acc = knn_model.score(X_val, y_val)        # Validation accuracy\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"KNN Model Accuracy (Validation): {accuracy_val:.4f}\")\n",
        "print(\"\\nClassification Report (KNN - Validation):\")\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "print(\"\\nConfusion Matrix (KNN - Validation):\")\n",
        "print(confusion_matrix(y_val, y_pred_val))\n",
        "\n",
        "\n",
        "# Make predictions (on the test set)\n",
        "y_pred_test = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model (on the test set)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "test_acc = knn_model.score(X_test, y_test)  # Test accuracy\n",
        "\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "print(f\"KNN Model Accuracy (Test): {accuracy_test:.4f}\")\n",
        "print(\"\\nClassification Report (KNN - Test):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(\"\\nConfusion Matrix (KNN - Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n"
      ],
      "metadata": {
        "id": "bGcf-nKCq877"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "uuXTQPQ_o_D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/balanced_twitter_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"account_type\"])\n",
        "y = df[\"account_type\"]\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.33, random_state=42, stratify=y_train_temp\n",
        ")\n",
        "\n",
        "# Create a pipeline with scaling, feature selection, and Logistic Regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('feature_selector', SelectKBest(score_func=f_classif, k='all')),  # Use all features\n",
        "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
        "    'classifier__penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'classifier__solver': ['liblinear'],  # Works well for small datasets\n",
        "}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Training Logistic Regression with hyperparameter tuning...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from grid search\n",
        "best_lr = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred_val = best_lr.predict(X_val)\n",
        "y_pred_test = best_lr.predict(X_test)\n",
        "\n",
        "# Probability estimates for ROC curve\n",
        "y_proba_val = best_lr.predict_proba(X_val)[:, 1]\n",
        "y_proba_test = best_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "train_acc = best_lr.score(X_train, y_train)\n",
        "val_acc = best_lr.score(X_val, y_val)\n",
        "test_acc = best_lr.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# AUC-ROC Score\n",
        "print(f\"\\nValidation AUC-ROC: {roc_auc_score(y_val, y_proba_val):.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc_score(y_test, y_proba_test):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "print(\"\\nConfusion Matrix (Validation):\")\n",
        "print(confusion_matrix(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(\"\\nConfusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Feature Importance (coefficients)\n",
        "feature_importance = best_lr.named_steps['classifier'].coef_[0]\n",
        "feature_names = X.columns\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': feature_importance\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "# Plot top 15 important features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'][:15], importance_df['Coefficient'][:15], color='skyblue')\n",
        "plt.xlabel('Coefficient Value (Absolute Magnitude)')\n",
        "plt.title('Top 15 Important Features (Logistic Regression)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_proba_test):.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k5BcOTGnpKvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "-SjafAKvpBVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/balanced_twitter_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"account_type\"])\n",
        "y = df[\"account_type\"]\n",
        "\n",
        "# Encode labels (if not already numerical)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)  # 0 for human, 1 for bot\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape data for CNN (samples, timesteps, features)\n",
        "# We'll treat each feature as a timestep in 1D convolution\n",
        "X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_reshaped, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.33, random_state=42, stratify=y_train_temp\n",
        ")\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_val_cat = to_categorical(y_val)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# CNN Model Architecture\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu',\n",
        "           input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(2, activation='softmax')  # 2 output classes\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train_cat, verbose=0)\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_val = np.argmax(model.predict(X_val), axis=1)\n",
        "y_pred_test = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "print(\"\\nConfusion Matrix (Validation):\")\n",
        "print(confusion_matrix(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(\"\\nConfusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SpKWe_CNpF32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "-IyuY2xvpCgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/balanced_twitter_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"account_type\"])\n",
        "y = df[\"account_type\"]\n",
        "\n",
        "# Encode labels (if not already numerical)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)  # 0 for human, 1 for bot (or vice versa)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape data for LSTM (samples, timesteps, features)\n",
        "# Since we don't have natural sequences, we'll treat each feature as a timestep\n",
        "X_reshaped = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_reshaped, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.33, random_state=42, stratify=y_train_temp\n",
        ")\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_val_cat = to_categorical(y_val)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# LSTM Model Architecture\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),  # Single timestep, many features\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(2, activation='softmax')  # 2 output classes\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train_cat, verbose=0)\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=0)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_val = np.argmax(model.predict(X_val), axis=1)\n",
        "y_pred_test = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "print(\"\\nConfusion Matrix (Validation):\")\n",
        "print(confusion_matrix(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(\"\\nConfusion Matrix (Test):\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eBB921jBpH4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}